{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "1ttb5uFdED3y"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "import datetime\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import TextVectorization, Input, Embedding, SimpleRNN, Dense, Bidirectional, LSTM, Dropout, GRU, Conv1D, Flatten, MultiHeadAttention, LayerNormalization\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras import Layer\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import gensim.downloader as api\n",
        "from tensorboard.plugins import projector"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds, val_ds, test_ds = tfds.load(\n",
        "    \"imdb_reviews\",\n",
        "    split=[\"train\", \"test[:50%]\", \"test[50%:]\"],\n",
        "    as_supervised=True\n",
        ")"
      ],
      "metadata": {
        "id": "ah9qv1BzEMR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds"
      ],
      "metadata": {
        "id": "zDN36SCvEtvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for review, label in train_ds.take(2):\n",
        "  print(review)\n",
        "  print(label)"
      ],
      "metadata": {
        "id": "GLqzr_tuFFVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def standardization(input_data):\n",
        "  lower_case = tf.strings.lower(input_data)\n",
        "  no_tag = tf.strings.regex_replace(lower_case, \"<[^>]+>\", \"\") # remove html tag from text\n",
        "  output = tf.strings.regex_replace(no_tag, \"[%s]\"%re.escape(string.punctuation), \"\") # remove punctuation\n",
        "\n",
        "  return output"
      ],
      "metadata": {
        "id": "oKAYPLa1FFXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOCB_SIZE = 10000\n",
        "SEQUENCE_LENGTH = 250\n",
        "BATCH_SIZE = 64\n",
        "EMBEDING_DIM = 300"
      ],
      "metadata": {
        "id": "4o8248bwk8Eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer = TextVectorization(\n",
        "    standardize=standardization,\n",
        "    max_tokens=VOCB_SIZE,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=SEQUENCE_LENGTH\n",
        ")"
      ],
      "metadata": {
        "id": "gnbff-9YFFZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lengths = []\n",
        "# words = []\n",
        "\n",
        "# for review, label in train_ds.take(10):\n",
        "#   for word in tf.strings.split(review, sep=\" \"):\n",
        "#     if word in words:\n",
        "#       pass\n",
        "#     else:\n",
        "#       words.append(word)\n",
        "#   lengths.append(len(tf.strings.split(review, sep=\" \")))"
      ],
      "metadata": {
        "id": "A8e8SVancHJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(training_data)"
      ],
      "metadata": {
        "id": "ri3FL_jqmhBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer.get_vocabulary()"
      ],
      "metadata": {
        "id": "XIouxdNVmhDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vectorize_layer.get_vocabulary())"
      ],
      "metadata": {
        "id": "72BQsWtamhIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for review, label in train_ds.take(1):\n",
        "  print(review)\n",
        "  print(label)"
      ],
      "metadata": {
        "id": "kgSvQeK8mhKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorizer(review, label):\n",
        "  return vectorize_layer(review), label"
      ],
      "metadata": {
        "id": "dyz2yIffodKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_ds.map(vectorizer)\n",
        "val_dataset = val_ds.map(vectorizer)"
      ],
      "metadata": {
        "id": "tHFvO40iodMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for review, label in train_dataset.take(1):\n",
        "  print(review)\n",
        "  print(label)"
      ],
      "metadata": {
        "id": "g3AjTC9LodO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "q_g9kM1modRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDING_DIM = 64\n",
        "model = tf.keras.Sequential([\n",
        "    Input(shape=(SEQUENCE_LENGTH,)),\n",
        "    Embedding(VOCB_SIZE, EMBEDING_DIM),\n",
        "    SimpleRNN(32),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "vEioVSr_qT-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDING_DIM = 64\n",
        "model = tf.keras.Sequential([\n",
        "    Input(shape=(SEQUENCE_LENGTH,)),\n",
        "    Embedding(VOCB_SIZE, EMBEDING_DIM),\n",
        "\n",
        "    Bidirectional(LSTM(64, return_sequences=True)),\n",
        "    Bidirectional(LSTM(32)),\n",
        "\n",
        "    Dense(64, activation=\"relu\"),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "o-zttfhPtcTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDING_DIM = 64\n",
        "model = tf.keras.Sequential([\n",
        "    Input(shape=(SEQUENCE_LENGTH,)),\n",
        "    Embedding(VOCB_SIZE, EMBEDING_DIM),\n",
        "\n",
        "    Bidirectional(GRU(64, return_sequences=True)),\n",
        "    Bidirectional(GRU(32)),\n",
        "\n",
        "    Dense(64, activation=\"relu\"),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "--mI61DZ1XcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM=300\n",
        "model=tf.keras.models.Sequential([\n",
        "    Input(shape=(SEQUENCE_LENGTH,)),\n",
        "    Embedding(VOCB_SIZE,EMBEDDING_DIM),\n",
        "\n",
        "    Conv1D(32, 3, activation='relu',),\n",
        "    Flatten(),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1,activation='sigmoid'),\n",
        "])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "JmJGB4eW9942"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_filepath = '/content/rnn.weights.h5'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)"
      ],
      "metadata": {
        "id": "NipYQSUvqTWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "OhbHSGaqqTY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_dir = 'logs/imdb/fit'+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '/'"
      ],
      "metadata": {
        "id": "MpxnE8eMNb7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
      ],
      "metadata": {
        "id": "HRdAcftTN6BC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_dataset, validation_data=val_dataset, epochs=5, callbacks=[tensorboard_callback])"
      ],
      "metadata": {
        "id": "AloUj3i-qTbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join(log_dir,'metadata.tsv'),\"w\",encoding=\"utf-8\") as f:\n",
        "  for i in range(VOCB_SIZE):\n",
        "    f.write(\"{} {}\\n\".format(i,vectorize_layer.get_vocabulary()[i]))"
      ],
      "metadata": {
        "id": "iXl5By19P7ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_weights=tf.Variable(model.layers[0].get_weights()[0])\n",
        "print(embedding_weights.shape)"
      ],
      "metadata": {
        "id": "lW9fDq9-QX8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint=tf.train.Checkpoint(embedding=embedding_weights)\n",
        "checkpoint.save(os.path.join(log_dir,\"embedding.ckpt\"))\n",
        "\n",
        "config=projector.ProjectorConfig()\n",
        "embedding=config.embeddings.add()"
      ],
      "metadata": {
        "id": "xlOTDQLDS032"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding.metadata_path='metadata.tsv'\n",
        "projector.visualize_embeddings(log_dir,config)"
      ],
      "metadata": {
        "id": "PCcCGAo6Wnzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "2fj1NzlyW8XZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs/imdb/fit/"
      ],
      "metadata": {
        "id": "-tLUA9ifW-8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title(\"model_loss\")\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['tain', \"val\"], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SImhUVf_zXTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title(\"model_accuracy\")\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(['tain', \"val\"], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8wMDO_GK1BGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec = api.load(\"word2vec-google-news-300\")"
      ],
      "metadata": {
        "id": "MSoJf8p11BIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec.vectors.shape"
      ],
      "metadata": {
        "id": "mAyt2njV-hDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec.key_to_index"
      ],
      "metadata": {
        "id": "wdv2hOYI-hF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(word2vec[\"The\"])"
      ],
      "metadata": {
        "id": "lny3HHiX-hIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec.most_similar('Man')"
      ],
      "metadata": {
        "id": "bYmtbAFz-hTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def first_cap(word):\n",
        "  return word[0].upper() + word[1:]"
      ],
      "metadata": {
        "id": "XAe5JqRYB8eL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_embedding = []\n",
        "for i in range(len(vectorize_layer.get_vocabulary())):\n",
        " try:\n",
        "  pretrained_embedding.append(word2vec[vectorize_layer.get_vocabulary()[i]])\n",
        " except:\n",
        "  print(vectorize_layer.get_vocabulary()[i])\n",
        "  try:\n",
        "    pretrained_embedding.append(word2vec[first_cap(vectorize_layer.get_vocabulary()[i])])\n",
        "    print(\"toupper\")\n",
        "  except:\n",
        "    print(\"nosloution\")\n",
        "    pretrained_embedding.append(random.normal(loc=0,scale=1,size=(EMBEDING_DIM,)))\n",
        "\n",
        "  if i%1000==0:\n",
        "    print(\"i is=== \", i)"
      ],
      "metadata": {
        "id": "OryO73vF_w3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_embedding_array = np.array(pretrained_embedding)"
      ],
      "metadata": {
        "id": "yusI2UxT_w6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDING_DIM = 300\n",
        "model = tf.keras.Sequential([\n",
        "    Input(shape=(SEQUENCE_LENGTH,)),\n",
        "    Embedding(VOCB_SIZE, EMBEDING_DIM, embeddings_initializer=tf.keras.initializers.Constant(pretrained_embedding), trainable=True),\n",
        "\n",
        "    Conv1D(32, 3, activation=\"relu\"),\n",
        "    Flatten(),\n",
        "\n",
        "    Dense(64, activation=\"relu\"),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "CDHl0KSbDyy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Z9xPgWiIDzCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=10,)"
      ],
      "metadata": {
        "id": "RR3dIObaHbmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data=tf.data.Dataset.from_tensor_slices([[\"this movie looks very interesting, i love the fact that the actors do a great job in showing how people lived in the 18th century, which wasn't very good at all. But atleast this movie recreates this scenes! \"],\n",
        "                                              [\"very good start, but movie started becoming interesting at some point and fortunately at some point it started becoming much more fun, though there was too much background noise, so in all i liked this movie \"],])\n"
      ],
      "metadata": {
        "id": "QHXdBx6vHbpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorizer_test(review):\n",
        "    return vectorize_layer(review)\n",
        "test_dataset=test_data.map(vectorizer_test)"
      ],
      "metadata": {
        "id": "5hL1k2zPHbrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(test_dataset)"
      ],
      "metadata": {
        "id": "pIjQABcEIV8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = Input(shape=(1,), dtype=\"string\")\n",
        "vectorized_inputs=vectorize_layer(inputs)\n",
        "outputs = model(vectorized_inputs)\n",
        "inference_ready_model = tf.keras.Model(inputs, outputs)\n",
        "inference_ready_model.summary()"
      ],
      "metadata": {
        "id": "iPqeh4MQKkd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inference_ready_model.predict([\"this movie looks very interesting, i love the fact that the actors do a great job in showing how people lived in the 18th century, which wasn't very good at all. But atleast this movie recreates this scenes! \",\n",
        "                               \"very good start, but movie started becoming interesting at some point and fortunately at some point it started becoming much more fun, though there was too much background noise, so in all i liked this movie \"])"
      ],
      "metadata": {
        "id": "Qtd9oWwfLHk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def positional_encoding(model_size,SEQUENCE_LENGTH):\n",
        "  output=[]\n",
        "  for pos in range(SEQUENCE_LENGTH):\n",
        "    PE=np.zeros((model_size))\n",
        "    for i in range(model_size):\n",
        "      if i%2==0:\n",
        "        PE[i]=np.sin(pos/(10000**(i/model_size)))\n",
        "      else:\n",
        "        PE[i]=np.cos(pos/(10000**((i-1)/model_size)))\n",
        "    output.append(tf.expand_dims(PE,axis=0))\n",
        "  out=tf.concat(output,axis=0)\n",
        "  out=tf.expand_dims(out,axis=0)\n",
        "  return tf.cast(out,dtype=tf.float32)"
      ],
      "metadata": {
        "id": "TJnBXjJ6IWBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(Layer):\n",
        "  def __init__(self, sequence_length, vocab_size, embed_dim,):\n",
        "    super(Embeddings, self).__init__()\n",
        "    self.token_embeddings=Embedding(\n",
        "        input_dim=vocab_size, output_dim=embed_dim)\n",
        "    self.sequence_length = sequence_length\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embed_dim = embed_dim\n",
        "\n",
        "  def call(self, inputs):\n",
        "    embedded_tokens = self.token_embeddings(inputs)\n",
        "    embedded_positions=positional_encoding(\n",
        "        self.embed_dim,self.sequence_length)\n",
        "    return embedded_tokens + embedded_positions\n",
        "\n",
        "  def compute_mask(self, inputs, mask=None):\n",
        "    return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "  def get_config(self):\n",
        "      config = super().get_config()\n",
        "      config.update({\n",
        "        \"sequence_length\": self.sequence_length,\n",
        "        \"vocab_size\": self.vocab_size,\n",
        "        \"embed_dim\": self.embed_dim,\n",
        "      })\n",
        "      return config\n"
      ],
      "metadata": {
        "id": "Ucipha2Vk4MR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads,):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim,\n",
        "        )\n",
        "        self.dense_proj=tf.keras.Sequential(\n",
        "            [Dense(dense_dim, activation=\"relu\"),Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = LayerNormalization()\n",
        "        self.layernorm_2 = LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "      if mask is not None:\n",
        "        mask1 = mask[:, :, tf.newaxis]\n",
        "        mask2 = mask[:,tf.newaxis, :]\n",
        "        padding_mask = tf.cast(mask1&mask2, dtype=\"int32\")\n",
        "\n",
        "      attention_output = self.attention(\n",
        "          query=inputs, key=inputs,value=inputs,attention_mask=padding_mask\n",
        "      )\n",
        "\n",
        "      proj_input = self.layernorm_1(inputs + attention_output)\n",
        "      proj_output = self.dense_proj(proj_input)\n",
        "      return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "      config = super().get_config()\n",
        "      config.update({\n",
        "        \"embed_dim\": self.embed_dim,\n",
        "        \"num_heads\": self.num_heads,\n",
        "        \"dense_dim\": self.dense_dim,\n",
        "      })\n",
        "      return config"
      ],
      "metadata": {
        "id": "rwWfA7vjk4Oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM=128\n",
        "D_FF=1024\n",
        "NUM_HEADS=8\n",
        "NUM_LAYERS=1\n",
        "NUM_EPOCHS=20"
      ],
      "metadata": {
        "id": "gTEHQQykk4Qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input=Input(shape=(None,), dtype=\"int64\", name=\"input\")\n",
        "x = Embeddings(SEQUENCE_LENGTH,VOCB_SIZE,EMBEDDING_DIM)(encoder_input)\n",
        "\n",
        "for _ in range(NUM_LAYERS):\n",
        "  x=TransformerEncoder(EMBEDDING_DIM,D_FF,NUM_HEADS)(x)\n",
        "\n",
        "x = Flatten()(x)\n",
        "output=Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "transformer = tf.keras.Model(\n",
        "    encoder_input, output, name=\"transformer\"\n",
        ")\n",
        "transformer.summary()"
      ],
      "metadata": {
        "id": "AmSaI9t1k4S7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "K7GBz32Jk4Vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=transformer.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=10,)"
      ],
      "metadata": {
        "id": "FyFp4rXnk4YJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6yDqTA5wk4aW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}