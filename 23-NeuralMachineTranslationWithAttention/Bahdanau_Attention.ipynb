{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense,Bidirectional,GRU,Dropout,Input, Embedding,TextVectorization, LSTM\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "MOf_52PKeimR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.manythings.org/anki/fra-eng.zip"
      ],
      "metadata": {
        "id": "Y3Oh6Z6u3qBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/fra-eng.zip' -d '/content/dataset'"
      ],
      "metadata": {
        "id": "dnV-9JRA32pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_dataset = tf.data.TextLineDataset('/content/dataset/fra.txt')"
      ],
      "metadata": {
        "id": "K8KI14Xj4Cx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in text_dataset.take(3):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "hD24YQEr4ciN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE=20000\n",
        "ENGLISH_SEQUENCE_LENGTH=64\n",
        "FRENCH_SEQUENCE_LENGTH=64\n",
        "EMBEDDING_DIM=300\n",
        "BATCH_SIZE=64"
      ],
      "metadata": {
        "id": "b4RtMphc4ck9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_vectorize_layer=TextVectorization(\n",
        "    standardize='lower_and_strip_punctuation',\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=ENGLISH_SEQUENCE_LENGTH\n",
        ")"
      ],
      "metadata": {
        "id": "XVK5Dezm5fr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "french_vectorize_layer=TextVectorization(\n",
        "    standardize='lower_and_strip_punctuation',\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=FRENCH_SEQUENCE_LENGTH\n",
        ")"
      ],
      "metadata": {
        "id": "qyLhZsF05fuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def selector(input_text):\n",
        "  split_text=tf.strings.split(input_text,'\\t')\n",
        "  return {'input_1':split_text[0:1],'input_2':'starttoken '+split_text[1:2]},split_text[1:2]+' endtoken'"
      ],
      "metadata": {
        "id": "VKlff0Rg69CI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_dataset=text_dataset.map(selector)"
      ],
      "metadata": {
        "id": "0rXs1fffAyiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in split_dataset.take(3):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "Emk0BkUyLYQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def separator(input_text):\n",
        "  split_text=tf.strings.split(input_text,'\\t')\n",
        "  return split_text[0:1],'starttoken '+split_text[1:2]+' endtoken'"
      ],
      "metadata": {
        "id": "CPHPOS-KMXu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_dataset=text_dataset.map(separator)"
      ],
      "metadata": {
        "id": "WDFm5TlIM27h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in init_dataset.take(3):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "GOYPd65IM99L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_training_data=init_dataset.map(lambda x,y:x)\n",
        "english_vectorize_layer.adapt(english_training_data)"
      ],
      "metadata": {
        "id": "UXD5KCAX5zVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "french_training_data=init_dataset.map(lambda x,y:y)\n",
        "french_vectorize_layer.adapt(french_training_data)"
      ],
      "metadata": {
        "id": "KD5-LXJL5zYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorizer(inputs,output):\n",
        "  return {'input_1':english_vectorize_layer(inputs['input_1']),\n",
        "          'input_2':french_vectorize_layer(inputs['input_2'])},french_vectorize_layer(output)"
      ],
      "metadata": {
        "id": "xd2o0M-I5zbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=split_dataset.map(vectorizer)"
      ],
      "metadata": {
        "id": "BXKHzE6tOfQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in dataset.take(1):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "h4_vIgR_5zgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=dataset.shuffle(2048).unbatch().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "XE8GW1kBQQ69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_BATCHES=int(200000/BATCH_SIZE)"
      ],
      "metadata": {
        "id": "0xqcx669QARj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=dataset.take(int(0.9*NUM_BATCHES))\n",
        "val_dataset=dataset.skip(int(0.9*NUM_BATCHES))"
      ],
      "metadata": {
        "id": "JCgRjr9tQAUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_UNITS = 256"
      ],
      "metadata": {
        "id": "k2yZIgo0QAWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = Input(shape=(ENGLISH_SEQUENCE_LENGTH,), dtype=\"int64\", name=\"input_1\")\n",
        "x=Embedding(VOCAB_SIZE, EMBEDDING_DIM, )(input)\n",
        "encoded_input=Bidirectional(GRU(NUM_UNITS), )(x)\n",
        "\n",
        "shifted_target=Input(shape=(FRENCH_SEQUENCE_LENGTH,), dtype=\"int64\", name=\"input_2\")\n",
        "x=Embedding(VOCAB_SIZE,EMBEDDING_DIM,)(shifted_target)\n",
        "x = GRU(NUM_UNITS*2, return_sequences=True)(x, initial_state=encoded_input)\n",
        "\n",
        "x = Dropout(0.5)(x)\n",
        "target=Dense(VOCAB_SIZE,activation=\"softmax\")(x)\n",
        "seq2seq_gru=Model([input,shifted_target],target)\n",
        "seq2seq_gru.summary()"
      ],
      "metadata": {
        "id": "e3ox0vMHRJ16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BLEU(tf.keras.metrics.Metric):\n",
        "  def __init__(self, name='bleu_score'):\n",
        "    super(BLEU, self).__init__()\n",
        "    self.bleu_score = 0\n",
        "\n",
        "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "    y_pred = tf.argmax(y_pred, axis=-1)\n",
        "    self.bleu_score = 0\n",
        "    for i,j in zip(y_pred, y_true):\n",
        "      tf.autograph.experimental.set_loop_options() # add this line beacuse after removing element from j it give error.\n",
        "      total_word = tf.math.count_nonzero(i)\n",
        "      total_matches = 0\n",
        "      for word in i:\n",
        "        if word==0:\n",
        "          break\n",
        "        for q in range(len(j)):\n",
        "          if j[q] == 0:\n",
        "            break\n",
        "          if j[q] == word:\n",
        "            total_matches += 1\n",
        "            j = tf.boolean_mask(j, [False if y == q else True for y in range(len(j))])\n",
        "            break\n",
        "    self.bleu_score += total_matches/total_word\n",
        "\n",
        "  def result(self):\n",
        "    return self.bleu_score/BATCH_SIZE"
      ],
      "metadata": {
        "id": "mTB2zpp-hQl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq_gru.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    optimizer=tf.keras.optimizers.Adam(5e-4), metrics=[BLEU()])"
      ],
      "metadata": {
        "id": "NyoqRFsoRJ5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=seq2seq_gru.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=15,)"
      ],
      "metadata": {
        "id": "jqa4MyJARJ7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model_loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PyuCn72sQAbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "\n",
        "plt.title('model_accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lydOaiBSBu8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_word={x:y for x, y in zip(range(len(french_vectorize_layer.get_vocabulary())),\n",
        "                                   french_vectorize_layer.get_vocabulary())}"
      ],
      "metadata": {
        "id": "eqvP2NaXlJGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translator(english_sentence):\n",
        "  tokenized_english_sentence=english_vectorize_layer([english_sentence])\n",
        "  shifted_target = 'starttoken'\n",
        "\n",
        "  for i in range(FRENCH_SEQUENCE_LENGTH):\n",
        "    tokenized_shifted_target=french_vectorize_layer([shifted_target])\n",
        "\n",
        "    output = seq2seq_gru.predict([tokenized_english_sentence,tokenized_shifted_target])\n",
        "    french_word_imdex = tf.argmax(output, axis=-1)[0][i].numpy()\n",
        "    current_word = index_to_word[french_word_imdex]\n",
        "\n",
        "    if current_word == 'endtoken':\n",
        "      break\n",
        "\n",
        "    shifted_target += ' ' + current_word\n",
        "\n",
        "  return shifted_target[11:]"
      ],
      "metadata": {
        "id": "1M369RBLfKnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator('What makes you think that is not true?')"
      ],
      "metadata": {
        "id": "LKNalb9TfKpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, units):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.units = units\n",
        "    self.vocab_size = vocab_size\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.embedding = Embedding(self.vocab_size, self.embedding_dim)\n",
        "    self.lstm = LSTM(self.units, return_sequences=True)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.embedding(x)\n",
        "    output = self.lstm(x)\n",
        "    return output"
      ],
      "metadata": {
        "id": "HaYGA4s-N-UW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HIDDEN_UNITS = 256\n",
        "EMBEDDING_DIM = 256\n",
        "encoder = Encoder(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_UNITS)\n",
        "encoder_output = encoder(tf.zeros((128, 64)))\n",
        "encoder_output.shape"
      ],
      "metadata": {
        "id": "SzlOuwRQN-Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.units = units\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.w_1 = Dense(self.units)\n",
        "    self.w_2 = Dense(self.units)\n",
        "    self.w = Dense(1)\n",
        "\n",
        "  def call(self, prev_dec_state, enc_state):\n",
        "    scores = self.w(\n",
        "        tf.nn.tanh(\n",
        "            self.w_1(tf.expand_dims(prev_dec_state, -2)) + self.w_2(enc_state)\n",
        "        ))\n",
        "    attention_weights = tf.nn.softmax(scores, axis=1)\n",
        "    context_vector = attention_weights*enc_state\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "    return context_vector"
      ],
      "metadata": {
        "id": "6hEF1NywN-oE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bahdanau_attention = BahdanauAttention(256)\n",
        "context_vector = bahdanau_attention(tf.zeros((128, 32)), tf.zeros((128, 64, 32)))\n",
        "context_vector.shape"
      ],
      "metadata": {
        "id": "2K4QgYtQSMR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self,vocab_size,embedding_dim,dec_units,sequence_length):\n",
        "    super(Decoder,self).__init__()\n",
        "    self.embedding_dim=embedding_dim\n",
        "    self.vocab_size=vocab_size\n",
        "    self.dec_units=dec_units\n",
        "    self.sequence_length=sequence_length\n",
        "\n",
        "  def build(self,input_shape):\n",
        "    self.dense=Dense(self.vocab_size,activation=\"softmax\")\n",
        "    self.gru=GRU(\n",
        "        self.dec_units,return_sequences=True,return_state=True)\n",
        "    self.attention=BahdanauAttention(self.dec_units)\n",
        "    self.embedding=Embedding(self.vocab_size,self.embedding_dim)\n",
        "\n",
        "  def call(self,x,hidden,shifted_target):\n",
        "    outputs=[]\n",
        "    context_vectors=[]\n",
        "    shifted_target=self.embedding(shifted_target)\n",
        "\n",
        "    for t in range(0,self.sequence_length):\n",
        "      context_vector=self.attention(hidden,x)\n",
        "      dec_input=context_vector+shifted_target[:,t]\n",
        "      output,hidden=self.gru(tf.expand_dims(dec_input,1))\n",
        "      outputs.append(output[:,0])\n",
        "\n",
        "    outputs=tf.convert_to_tensor(outputs)\n",
        "    outputs=tf.transpose(outputs, perm=[1,0,2])\n",
        "\n",
        "    outputs=self.dense(outputs)\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "AlvsP0T9SMWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder=Decoder(VOCAB_SIZE,EMBEDDING_DIM,HIDDEN_UNITS,FRENCH_SEQUENCE_LENGTH)\n",
        "outputs=decoder(encoder_output,tf.zeros([128,HIDDEN_UNITS]),tf.zeros([128,64]))\n",
        "outputs.shape"
      ],
      "metadata": {
        "id": "ue6zuoBASMYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### ENCODER\n",
        "input = Input(shape=(ENGLISH_SEQUENCE_LENGTH,), dtype=\"int64\", name=\"input_1\")\n",
        "encoder=Encoder(VOCAB_SIZE,EMBEDDING_DIM,HIDDEN_UNITS)\n",
        "encoder_output=encoder(input)\n",
        "\n",
        "### DECODER\n",
        "shifted_target=Input(shape=(FRENCH_SEQUENCE_LENGTH,), dtype=\"int64\", name=\"input_2\")\n",
        "decoder=Decoder(VOCAB_SIZE,EMBEDDING_DIM,HIDDEN_UNITS,FRENCH_SEQUENCE_LENGTH)\n",
        "decoder_output,attention_weightss=decoder(encoder_output,tf.zeros([1,HIDDEN_UNITS]),shifted_target)\n",
        "\n",
        "### OUTPUT\n",
        "bahdanau=Model([input,shifted_target],decoder_output)\n",
        "bahdanau.summary()"
      ],
      "metadata": {
        "id": "hXwcj2PaSMap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bahdanau.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    optimizer=tf.keras.optimizers.Adam(5e-4),)"
      ],
      "metadata": {
        "id": "wDCh3YDLb4V3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=bahdanau.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=30,)"
      ],
      "metadata": {
        "id": "xYIJmdK-b4Ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cn62aKRXb4cT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J7ZWDNKSb4eT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}