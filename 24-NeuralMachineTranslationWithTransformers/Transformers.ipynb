{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense,Bidirectional,GRU,Dropout,Input, Embedding,TextVectorization, LSTM, MultiHeadAttention, LayerNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "MOf_52PKeimR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.manythings.org/anki/fra-eng.zip"
      ],
      "metadata": {
        "id": "Y3Oh6Z6u3qBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/fra-eng.zip' -d '/content/dataset'"
      ],
      "metadata": {
        "id": "dnV-9JRA32pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_dataset = tf.data.TextLineDataset('/content/dataset/fra.txt')"
      ],
      "metadata": {
        "id": "K8KI14Xj4Cx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in text_dataset.take(3):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "hD24YQEr4ciN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE=20000\n",
        "ENGLISH_SEQUENCE_LENGTH=64\n",
        "FRENCH_SEQUENCE_LENGTH=64\n",
        "EMBEDDING_DIM=300\n",
        "BATCH_SIZE=64"
      ],
      "metadata": {
        "id": "b4RtMphc4ck9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_vectorize_layer=TextVectorization(\n",
        "    standardize='lower_and_strip_punctuation',\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=ENGLISH_SEQUENCE_LENGTH\n",
        ")"
      ],
      "metadata": {
        "id": "XVK5Dezm5fr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "french_vectorize_layer=TextVectorization(\n",
        "    standardize='lower_and_strip_punctuation',\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=FRENCH_SEQUENCE_LENGTH\n",
        ")"
      ],
      "metadata": {
        "id": "qyLhZsF05fuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def selector(input_text):\n",
        "  split_text=tf.strings.split(input_text,'\\t')\n",
        "  return {'input_1':split_text[0:1],'input_2':'starttoken '+split_text[1:2]},split_text[1:2]+' endtoken'"
      ],
      "metadata": {
        "id": "VKlff0Rg69CI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_dataset=text_dataset.map(selector)"
      ],
      "metadata": {
        "id": "0rXs1fffAyiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in split_dataset.take(3):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "Emk0BkUyLYQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def separator(input_text):\n",
        "  split_text=tf.strings.split(input_text,'\\t')\n",
        "  return split_text[0:1],'starttoken '+split_text[1:2]+' endtoken'"
      ],
      "metadata": {
        "id": "CPHPOS-KMXu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_dataset=text_dataset.map(separator)"
      ],
      "metadata": {
        "id": "WDFm5TlIM27h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in init_dataset.take(3):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "GOYPd65IM99L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_training_data=init_dataset.map(lambda x,y:x)\n",
        "english_vectorize_layer.adapt(english_training_data)"
      ],
      "metadata": {
        "id": "UXD5KCAX5zVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "french_training_data=init_dataset.map(lambda x,y:y)\n",
        "french_vectorize_layer.adapt(french_training_data)"
      ],
      "metadata": {
        "id": "KD5-LXJL5zYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorizer(inputs,output):\n",
        "  return {'input_1':english_vectorize_layer(inputs['input_1']),\n",
        "          'input_2':french_vectorize_layer(inputs['input_2'])},french_vectorize_layer(output)"
      ],
      "metadata": {
        "id": "xd2o0M-I5zbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=split_dataset.map(vectorizer)"
      ],
      "metadata": {
        "id": "BXKHzE6tOfQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in dataset.take(1):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "h4_vIgR_5zgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=dataset.shuffle(2048).unbatch().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "XE8GW1kBQQ69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_BATCHES=int(200000/BATCH_SIZE)"
      ],
      "metadata": {
        "id": "0xqcx669QARj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=dataset.take(int(0.9*NUM_BATCHES))\n",
        "val_dataset=dataset.skip(int(0.9*NUM_BATCHES))"
      ],
      "metadata": {
        "id": "JCgRjr9tQAUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_UNITS = 256"
      ],
      "metadata": {
        "id": "k2yZIgo0QAWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = Input(shape=(ENGLISH_SEQUENCE_LENGTH,), dtype=\"int64\", name=\"input_1\")\n",
        "x=Embedding(VOCAB_SIZE, EMBEDDING_DIM, )(input)\n",
        "encoded_input=Bidirectional(GRU(NUM_UNITS), )(x)\n",
        "\n",
        "shifted_target=Input(shape=(FRENCH_SEQUENCE_LENGTH,), dtype=\"int64\", name=\"input_2\")\n",
        "x=Embedding(VOCAB_SIZE,EMBEDDING_DIM,)(shifted_target)\n",
        "x = GRU(NUM_UNITS*2, return_sequences=True)(x, initial_state=encoded_input)\n",
        "\n",
        "x = Dropout(0.5)(x)\n",
        "target=Dense(VOCAB_SIZE,activation=\"softmax\")(x)\n",
        "seq2seq_gru=Model([input,shifted_target],target)\n",
        "seq2seq_gru.summary()"
      ],
      "metadata": {
        "id": "e3ox0vMHRJ16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BLEU(tf.keras.metrics.Metric):\n",
        "  def __init__(self, name='bleu_score'):\n",
        "    super(BLEU, self).__init__()\n",
        "    self.bleu_score = 0\n",
        "\n",
        "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "    y_pred = tf.argmax(y_pred, axis=-1)\n",
        "    self.bleu_score = 0\n",
        "    for i,j in zip(y_pred, y_true):\n",
        "      tf.autograph.experimental.set_loop_options() # add this line beacuse after removing element from j it give error.\n",
        "      total_word = tf.math.count_nonzero(i)\n",
        "      total_matches = 0\n",
        "      for word in i:\n",
        "        if word==0:\n",
        "          break\n",
        "        for q in range(len(j)):\n",
        "          if j[q] == 0:\n",
        "            break\n",
        "          if j[q] == word:\n",
        "            total_matches += 1\n",
        "            j = tf.boolean_mask(j, [False if y == q else True for y in range(len(j))])\n",
        "            break\n",
        "    self.bleu_score += total_matches/total_word\n",
        "\n",
        "  def result(self):\n",
        "    return self.bleu_score/BATCH_SIZE"
      ],
      "metadata": {
        "id": "mTB2zpp-hQl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq_gru.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    optimizer=tf.keras.optimizers.Adam(5e-4), metrics=[BLEU()])"
      ],
      "metadata": {
        "id": "NyoqRFsoRJ5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=seq2seq_gru.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=15,)"
      ],
      "metadata": {
        "id": "jqa4MyJARJ7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model_loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PyuCn72sQAbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "\n",
        "plt.title('model_accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lydOaiBSBu8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_word={x:y for x, y in zip(range(len(french_vectorize_layer.get_vocabulary())),\n",
        "                                   french_vectorize_layer.get_vocabulary())}"
      ],
      "metadata": {
        "id": "eqvP2NaXlJGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translator(english_sentence):\n",
        "  tokenized_english_sentence=english_vectorize_layer([english_sentence])\n",
        "  shifted_target = 'starttoken'\n",
        "\n",
        "  for i in range(FRENCH_SEQUENCE_LENGTH):\n",
        "    tokenized_shifted_target=french_vectorize_layer([shifted_target])\n",
        "\n",
        "    output = seq2seq_gru.predict([tokenized_english_sentence,tokenized_shifted_target])\n",
        "    french_word_imdex = tf.argmax(output, axis=-1)[0][i].numpy()\n",
        "    current_word = index_to_word[french_word_imdex]\n",
        "\n",
        "    if current_word == 'endtoken':\n",
        "      break\n",
        "\n",
        "    shifted_target += ' ' + current_word\n",
        "\n",
        "  return shifted_target[11:]"
      ],
      "metadata": {
        "id": "1M369RBLfKnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator('What makes you think that is not true?')"
      ],
      "metadata": {
        "id": "LKNalb9TfKpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, units):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.units = units\n",
        "    self.vocab_size = vocab_size\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.embedding = Embedding(self.vocab_size, self.embedding_dim)\n",
        "    self.lstm = LSTM(self.units, return_sequences=True)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.embedding(x)\n",
        "    output = self.lstm(x)\n",
        "    return output"
      ],
      "metadata": {
        "id": "HaYGA4s-N-UW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HIDDEN_UNITS = 256\n",
        "EMBEDDING_DIM = 256\n",
        "encoder = Encoder(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_UNITS)\n",
        "encoder_output = encoder(tf.zeros((128, 64)))\n",
        "encoder_output.shape"
      ],
      "metadata": {
        "id": "SzlOuwRQN-Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.units = units\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.w_1 = Dense(self.units)\n",
        "    self.w_2 = Dense(self.units)\n",
        "    self.w = Dense(1)\n",
        "\n",
        "  def call(self, prev_dec_state, enc_state):\n",
        "    scores = self.w(\n",
        "        tf.nn.tanh(\n",
        "            self.w_1(tf.expand_dims(prev_dec_state, -2)) + self.w_2(enc_state)\n",
        "        ))\n",
        "    attention_weights = tf.nn.softmax(scores, axis=1)\n",
        "    context_vector = attention_weights*enc_state\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "    return context_vector"
      ],
      "metadata": {
        "id": "6hEF1NywN-oE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bahdanau_attention = BahdanauAttention(256)\n",
        "context_vector = bahdanau_attention(tf.zeros((128, 32)), tf.zeros((128, 64, 32)))\n",
        "context_vector.shape"
      ],
      "metadata": {
        "id": "2K4QgYtQSMR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self,vocab_size,embedding_dim,dec_units,sequence_length):\n",
        "    super(Decoder,self).__init__()\n",
        "    self.embedding_dim=embedding_dim\n",
        "    self.vocab_size=vocab_size\n",
        "    self.dec_units=dec_units\n",
        "    self.sequence_length=sequence_length\n",
        "\n",
        "  def build(self,input_shape):\n",
        "    self.dense=Dense(self.vocab_size,activation=\"softmax\")\n",
        "    self.gru=GRU(\n",
        "        self.dec_units,return_sequences=True,return_state=True)\n",
        "    self.attention=BahdanauAttention(self.dec_units)\n",
        "    self.embedding=Embedding(self.vocab_size,self.embedding_dim)\n",
        "\n",
        "  def call(self,x,hidden,shifted_target):\n",
        "    outputs=[]\n",
        "    context_vectors=[]\n",
        "    shifted_target=self.embedding(shifted_target)\n",
        "\n",
        "    for t in range(0,self.sequence_length):\n",
        "      context_vector=self.attention(hidden,x)\n",
        "      dec_input=context_vector+shifted_target[:,t]\n",
        "      output,hidden=self.gru(tf.expand_dims(dec_input,1))\n",
        "      outputs.append(output[:,0])\n",
        "\n",
        "    outputs=tf.convert_to_tensor(outputs)\n",
        "    outputs=tf.transpose(outputs, perm=[1,0,2])\n",
        "\n",
        "    outputs=self.dense(outputs)\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "AlvsP0T9SMWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self,vocab_size,embedding_dim,dec_units,sequence_length):\n",
        "    super(Decoder,self).__init__()\n",
        "    self.embedding_dim=embedding_dim\n",
        "    self.vocab_size=vocab_size\n",
        "    self.dec_units=dec_units\n",
        "    self.sequence_length=sequence_length\n",
        "\n",
        "  def build(self,input_shape):\n",
        "    self.dense=Dense(self.vocab_size,activation=\"softmax\")\n",
        "    self.gru=GRU(\n",
        "        self.dec_units,return_sequences=True,return_state=True)\n",
        "    self.attention=BahdanauAttention(self.dec_units)\n",
        "    self.embedding=Embedding(self.vocab_size,self.embedding_dim)\n",
        "\n",
        "  def call(self,x,hidden,shifted_target):\n",
        "    outputs=[]\n",
        "    context_vectors=[]\n",
        "    shifted_target=self.embedding(shifted_target)\n",
        "\n",
        "    for t in range(0,self.sequence_length):\n",
        "      context_vector=self.attention(hidden,x)\n",
        "      dec_input=context_vector+shifted_target[:,t]\n",
        "      # The original code used tf.expand_dims(dec_input,1) which caused the error\n",
        "      # Use tf.reshape to specify the shape explicitly\n",
        "      output,hidden=self.gru(tf.reshape(dec_input, [128, 1, dec_input.shape[1]]))\n",
        "      outputs.append(output[:,0])\n",
        "\n",
        "    outputs=tf.convert_to_tensor(outputs)\n",
        "    outputs=tf.transpose(outputs, perm=[1,0,2])\n",
        "\n",
        "    outputs=self.dense(outputs)\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "akeHUB5td5RS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder=Decoder(VOCAB_SIZE,EMBEDDING_DIM,HIDDEN_UNITS,FRENCH_SEQUENCE_LENGTH)\n",
        "outputs=decoder(encoder_output,tf.zeros([128,HIDDEN_UNITS]),tf.zeros([128,64]))\n",
        "outputs.shape"
      ],
      "metadata": {
        "id": "ue6zuoBASMYP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "outputId": "21ca4955-7a66-406e-f476-a36b08047604"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Exception encountered when calling Decoder.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'decoder_5' (of type Decoder). Either the `Decoder.call()` method is incorrect, or you need to implement the `Decoder.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nException encountered when calling GRU.call().\n\n\u001b[1mlen is not well defined for a symbolic Tensor (gru_5_1/Squeeze:0). Please call `x.shape` rather than `len(x)` for shape information.\u001b[0m\n\nArguments received by GRU.call():\n  • sequences=tf.Tensor(shape=(128, 1, 256), dtype=float32)\n  • initial_state=None\n  • mask=None\n  • training=False\u001b[0m\n\nArguments received by Decoder.call():\n  • args=('<KerasTensor shape=(None, 64, 256), dtype=float32, sparse=False, name=keras_tensor>', 'tf.Tensor(shape=(128, 256), dtype=float32)', 'tf.Tensor(shape=(128, 64), dtype=float32)')\n  • kwargs=<class 'inspect._empty'>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-5d7cd2466559>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mEMBEDDING_DIM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mHIDDEN_UNITS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFRENCH_SEQUENCE_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mHIDDEN_UNITS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-6f7fd6905983>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, hidden, shifted_target)\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0mcontext_vector\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0mdec_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext_vector\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mshifted_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m       \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m       \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Exception encountered when calling Decoder.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'decoder_5' (of type Decoder). Either the `Decoder.call()` method is incorrect, or you need to implement the `Decoder.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nException encountered when calling GRU.call().\n\n\u001b[1mlen is not well defined for a symbolic Tensor (gru_5_1/Squeeze:0). Please call `x.shape` rather than `len(x)` for shape information.\u001b[0m\n\nArguments received by GRU.call():\n  • sequences=tf.Tensor(shape=(128, 1, 256), dtype=float32)\n  • initial_state=None\n  • mask=None\n  • training=False\u001b[0m\n\nArguments received by Decoder.call():\n  • args=('<KerasTensor shape=(None, 64, 256), dtype=float32, sparse=False, name=keras_tensor>', 'tf.Tensor(shape=(128, 256), dtype=float32)', 'tf.Tensor(shape=(128, 64), dtype=float32)')\n  • kwargs=<class 'inspect._empty'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### ENCODER\n",
        "input = Input(shape=(ENGLISH_SEQUENCE_LENGTH,), dtype=\"int64\", name=\"input_1\")\n",
        "encoder=Encoder(VOCAB_SIZE,EMBEDDING_DIM,HIDDEN_UNITS)\n",
        "encoder_output=encoder(input)\n",
        "\n",
        "### DECODER\n",
        "shifted_target=Input(shape=(FRENCH_SEQUENCE_LENGTH,), dtype=\"int64\", name=\"input_2\")\n",
        "decoder=Decoder(VOCAB_SIZE,EMBEDDING_DIM,HIDDEN_UNITS,FRENCH_SEQUENCE_LENGTH)\n",
        "decoder_output,attention_weightss=decoder(encoder_output,tf.zeros([1,HIDDEN_UNITS]),shifted_target)\n",
        "\n",
        "### OUTPUT\n",
        "bahdanau=Model([input,shifted_target],decoder_output)\n",
        "bahdanau.summary()"
      ],
      "metadata": {
        "id": "hXwcj2PaSMap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bahdanau.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    optimizer=tf.keras.optimizers.Adam(5e-4),)"
      ],
      "metadata": {
        "id": "wDCh3YDLb4V3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=bahdanau.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=30,)"
      ],
      "metadata": {
        "id": "xYIJmdK-b4Ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def positional_encoding(model_size,SEQUENCE_LENGTH):\n",
        "  output=[]\n",
        "  for pos in range(SEQUENCE_LENGTH):\n",
        "    PE=np.zeros((model_size))\n",
        "    for i in range(model_size):\n",
        "      if i%2==0:\n",
        "        PE[i]=np.sin(pos/(10000**(i/model_size)))\n",
        "      else:\n",
        "        PE[i]=np.cos(pos/(10000**((i-1)/model_size)))\n",
        "    output.append(tf.expand_dims(PE,axis=0))\n",
        "  out=tf.concat(output,axis=0)\n",
        "  out=tf.expand_dims(out,axis=0)\n",
        "  return tf.cast(out,dtype=tf.float32)"
      ],
      "metadata": {
        "id": "Cn62aKRXb4cT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(tf.keras.Layer):\n",
        "  def __init__(self, sequence_length, vocab_size, embed_dim,):\n",
        "    super(Embeddings, self).__init__()\n",
        "    self.token_embeddings=Embedding(\n",
        "        input_dim=vocab_size, output_dim=embed_dim)\n",
        "    self.sequence_length = sequence_length\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embed_dim = embed_dim\n",
        "\n",
        "  def call(self, inputs):\n",
        "    embedded_tokens = self.token_embeddings(inputs)\n",
        "    embedded_positions=positional_encoding(\n",
        "        self.embed_dim,self.sequence_length)\n",
        "    return embedded_tokens + embedded_positions\n",
        "\n",
        "  def compute_mask(self, inputs, mask=None):\n",
        "    return tf.math.not_equal(inputs, 0)"
      ],
      "metadata": {
        "id": "Vtt0pr7UCRoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSelfAttention(tf.keras.Layer):\n",
        "  def __init__(self, model_size):\n",
        "    super(CustomMultiHeadAttention, self).__init__()\n",
        "    self.model_size = model_size\n",
        "\n",
        "  def call(self, query, key, value, masking):\n",
        "    score = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "    score /= tf.math.sqrt(tf.cast(self.model_size, tf.float32))\n",
        "\n",
        "    masking = tf.cast(masking, dtype=tf.float32)\n",
        "    score += (1.-masking) * -1e9\n",
        "    attention = tf.nn.softmax(score, axis=-1) * masking\n",
        "    head = tf.matmul(attention, value)\n",
        "\n",
        "    return head"
      ],
      "metadata": {
        "id": "OtZqBAOF8WMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomMultiHeadAttention(tf.keras.Layer):\n",
        "  def __init__(self, num_heads, key_dim):\n",
        "    super(CustomMultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.dense_q = [Dense(key_dim) for _ in range(num_heads)]\n",
        "    self.dense_k = [Dense(key_dim) for _ in range(num_heads)]\n",
        "    self.dense_v = [Dense(key_dim) for _ in range(num_heads)]\n",
        "    self.dense_o = Dense(key_dim)\n",
        "    self.self_attention = CustomSelfAttention(key_dim)\n",
        "\n",
        "  def call(self, query, key, value, attention_mask):\n",
        "    heads = []\n",
        "\n",
        "    for i in range(self.num_heads):\n",
        "      head = self.self_attention(self.dense_q[i](query), self.dense_k[i](key), self.dense_v[i](value), attention_mask)\n",
        "\n",
        "      heads.append(head)\n",
        "\n",
        "    heads = tf.concat(heads, axis=2)\n",
        "    heads = self.dense_o(heads)\n",
        "\n",
        "    return heads"
      ],
      "metadata": {
        "id": "WXo-su_eBdsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(tf.keras.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads,):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = CustomMultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim,\n",
        "        )\n",
        "        self.dense_proj=tf.keras.Sequential(\n",
        "            [Dense(dense_dim, activation=\"relu\"),\n",
        "             Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = LayerNormalization()\n",
        "        self.layernorm_2 = LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "\n",
        "      if mask is not None:\n",
        "        mask = tf.cast(\n",
        "            mask[:,tf.newaxis, :], dtype=\"int32\")\n",
        "        T = tf.shape(mask)[2]\n",
        "        padding_mask = tf.repeat(mask,T,axis=1)\n",
        "      attention_output = self.attention(\n",
        "          query=inputs, key=inputs,value=inputs,\n",
        "          attention_mask=padding_mask\n",
        "      )\n",
        "\n",
        "      proj_input = self.layernorm_1(inputs + attention_output)\n",
        "      proj_output = self.dense_proj(proj_input)\n",
        "      return self.layernorm_2(proj_input + proj_output)"
      ],
      "metadata": {
        "id": "XF3GXEwYCRst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(tf.keras.Layer):\n",
        "  def __init__(self, embed_dim, latent_dim, num_heads,):\n",
        "    super(TransformerDecoder, self).__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "    self.latent_dim = latent_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.attention_1=CustomMultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embed_dim\n",
        "    )\n",
        "    self.attention_2=CustomMultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embed_dim\n",
        "    )\n",
        "    self.dense_proj = tf.keras.Sequential(\n",
        "        [Dense(latent_dim, activation=\"relu\"),Dense(embed_dim),]\n",
        "    )\n",
        "    self.layernorm_1=LayerNormalization()\n",
        "    self.layernorm_2=LayerNormalization()\n",
        "    self.layernorm_3=LayerNormalization()\n",
        "    self.supports_masking = True\n",
        "  def call(self, inputs, encoder_outputs, enc_mask, mask=None):\n",
        "\n",
        "\n",
        "    if mask is not None:\n",
        "      causal_mask=tf.linalg.band_part(\n",
        "        tf.ones([tf.shape(inputs)[0],\n",
        "                 tf.shape(inputs)[1],\n",
        "                 tf.shape(inputs)[1]],dtype=tf.int32),-1,0)\n",
        "      mask = tf.cast(\n",
        "          mask[:,tf.newaxis, :], dtype=\"int32\")\n",
        "      enc_mask = tf.cast(\n",
        "          enc_mask[:,tf.newaxis, :], dtype=\"int32\")\n",
        "      T = tf.shape(mask)[2]\n",
        "      padding_mask = tf.repeat(mask,T,axis=1)\n",
        "      cross_attn_mask = tf.repeat(enc_mask,T,axis=1)\n",
        "      combined_mask=tf.minimum(padding_mask,causal_mask)\n",
        "\n",
        "    attention_output_1 = self.attention_1(\n",
        "        query=inputs,key=inputs,value=inputs,\n",
        "        attention_mask=combined_mask,\n",
        "\n",
        "    )\n",
        "\n",
        "    out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "    attention_output_2, scores = self.attention_2(\n",
        "        query=out_1,key=encoder_outputs,value=encoder_outputs,\n",
        "        attention_mask=cross_attn_mask,\n",
        "        return_attention_scores=True\n",
        "    )\n",
        "    out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "    proj_output = self.dense_proj(out_2)\n",
        "    return self.layernorm_3(out_2 + proj_output), scores"
      ],
      "metadata": {
        "id": "DFTw7wO4LdOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM=512\n",
        "D_FF=2048\n",
        "NUM_HEADS=8\n",
        "NUM_LAYERS=1\n",
        "NUM_EPOCHS=10\n",
        "attention_scores = {}"
      ],
      "metadata": {
        "id": "rI0HcsQyLdQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_inputs=Input(shape=(None,), dtype=\"int64\", name=\"input_1\")\n",
        "emb = Embeddings(ENGLISH_SEQUENCE_LENGTH,VOCAB_SIZE,EMBEDDING_DIM)\n",
        "x = emb(encoder_inputs)\n",
        "enc_mask = emb.compute_mask(encoder_inputs)\n",
        "\n",
        "for _ in range(NUM_LAYERS):\n",
        "  x=TransformerEncoder(EMBEDDING_DIM,D_FF,NUM_HEADS)(x)\n",
        "encoder_outputs=x\n",
        "\n",
        "decoder_inputs=Input(shape=(None,), dtype=\"int64\", name=\"input_2\")\n",
        "\n",
        "x = Embeddings(FRENCH_SEQUENCE_LENGTH,VOCAB_SIZE,EMBEDDING_DIM)(decoder_inputs)\n",
        "for i in range(NUM_LAYERS):\n",
        "  x,scores=TransformerDecoder(EMBEDDING_DIM,D_FF,NUM_HEADS)(x, encoder_outputs,enc_mask)\n",
        "  attention_scores[f'decoder_layer{i+1}_block2'] = scores\n",
        "\n",
        "decoder_outputs=Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
        "\n",
        "attention_score_model = tf.keras.Model(\n",
        "    [encoder_input, decoder_input], attention_scores, name=\"attention_score_model\"\n",
        ")\n",
        "\n",
        "transformer = tf.keras.Model(\n",
        "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
        ")\n",
        "transformer.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "wEDt5pfmLdTW",
        "outputId": "9fb28055-60ac-470a-adc9-13f9a642df69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-cbf7605dfd06>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mencoder_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"int64\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"input_1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mENGLISH_SEQUENCE_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mEMBEDDING_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0menc_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-248eb1d70448>\u001b[0m in \u001b[0;36mcompute_mask\u001b[0;34m(self, inputs, mask)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Scheduler(LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps):\n",
        "    super(Scheduler, self).__init__()\n",
        "    self.d_model = tf.cast(d_model, tf.float64)\n",
        "    self.warmup_steps = tf.cast(warmup_steps, dtype=tf.float64)\n",
        "\n",
        "  def __call__(self, step):\n",
        "    step = tf.cast(step, dtype=tf.float64)\n",
        "    return (self.d_model**(-0.5))*tf.math.minimum(step**(-0.5), step * (self.warmup_steps ** -1.5))"
      ],
      "metadata": {
        "id": "J7ZWDNKSb4eT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WARM_UP_STEPS = 4000\n",
        "lr_scheduled = Scheduler(EMBEDDING_DIM, WARM_UP_STEPS)"
      ],
      "metadata": {
        "id": "p1PMVMknUwiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    optimizer = Adam(lr_scheduled, beta_1=0.9, beta_2=0.98, epsilon=1e-9),)"
      ],
      "metadata": {
        "id": "jVuT7U3IUwkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=transformer.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=10)"
      ],
      "metadata": {
        "id": "S_-3KOR4Uwmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize(english_sentence):\n",
        "  tokenized_english_sentence=english_vectorize_layer([english_sentence])\n",
        "  shifted_target = 'starttoken jal lai fait tres bien'\n",
        "\n",
        "  tokenized_shifted_target=french_vectorize_layer([shifted_target])\n",
        "  attention_weights = attention_score_model.predict([tokenized_english_sentence,tokenized_shifted_target])\n",
        "\n",
        "  return attention_weights\n",
        "\n",
        "out = visualize(\"I did it very well\")"
      ],
      "metadata": {
        "id": "vgnGzZ3BKvfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(8,6))\n",
        "plt.imshow(out['decoder_layer1_block2'][0][0][0:10,0:10])\n",
        "plt.title(\"Attention score\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "U4DKhEAEKvhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (12,12))\n",
        "\n",
        "for i in range(NUM_HEADS):\n",
        "  ax = plt.subplot(2,4, i+1)\n",
        "\n",
        "  plt.imshow(out['decoder_layer1_block2'][0][i][0:10,0:10])\n",
        "  plt.title(\"Attention Scores for head:->\"+str(i+1))"
      ],
      "metadata": {
        "id": "n98zypCCKvj8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}