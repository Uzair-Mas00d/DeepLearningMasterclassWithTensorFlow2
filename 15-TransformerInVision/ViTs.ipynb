{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYTHAIRwCuPz"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "glyvudblR4nO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d muhammadhananasghar/human-emotions-datasethes"
      ],
      "metadata": {
        "id": "GuDRt0T1SB8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/human-emotions-datasethes.zip -d \"/content/dataset/\""
      ],
      "metadata": {
        "id": "TIpo4lhTSQWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import InputLayer, Conv2D, BatchNormalization, MaxPool2D, Flatten, Dense, Dropout, RandomRotation, RandomFlip, RandomContrast, Resizing, Rescaling, MaxPooling2D, GlobalAveragePooling2D, Add, Activation, Input, Embedding, LayerNormalization, MultiHeadAttention, Permute\n",
        "from tensorflow.keras.regularizers import L2\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import CategoricalAccuracy, TopKCategoricalAccuracy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow.train import BytesList, FloatList, Int64List\n",
        "from tensorflow.train import Example, Features, Feature\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.metrics import confusion_matrix, roc_curve\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "9fkktBjX3v5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_directory = \"/content/dataset/Emotions Dataset/Emotions Dataset/train\"\n",
        "val_directory = \"/content/dataset/Emotions Dataset/Emotions Dataset/test\"\n",
        "CLASS_NAMES = [\"angry\", \"happy\", \"sad\"]\n",
        "\n",
        "CONFIGURATION = {\n",
        "    \"BATCH_SIZE\": 32,\n",
        "    \"IM_SIZE\": 256,\n",
        "    \"LEARNING_RATE\": 0.001,\n",
        "    \"N_EPOCHS\": 20,\n",
        "    \"DROPOUT_RATE\": 0.0,\n",
        "    \"REGULARIZATION_RATE\": 0.0,\n",
        "    \"N_FILTERS\": 6 ,\n",
        "    \"KERNEL_SIZE\": 3,\n",
        "    \"N_STRIDES\": 1,\n",
        "    \"POOL_SIZE\":2,\n",
        "    \"N_DENSE_1\": 100,\n",
        "    \"N_DENSE_2\": 10,\n",
        "    \"NUM_CLASSES\":3\n",
        "}"
      ],
      "metadata": {
        "id": "yLoaV32yTK75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    train_directory,\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"categorical\",\n",
        "    class_names=CLASS_NAMES,\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=CONFIGURATION[\"BATCH_SIZE\"],\n",
        "    image_size=(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]),\n",
        "    shuffle=True,\n",
        "    seed=99\n",
        ")\n",
        "\n",
        "val_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    val_directory,\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"categorical\",\n",
        "    class_names=CLASS_NAMES,\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=CONFIGURATION[\"BATCH_SIZE\"],\n",
        "    image_size=(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]),\n",
        "    shuffle=True,\n",
        "    seed=99\n",
        ")"
      ],
      "metadata": {
        "id": "YxrIXFaoS0O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in val_dataset.take(1):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "ke8Ex7grWpF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,12))\n",
        "\n",
        "for image, label in train_dataset.take(1):\n",
        "  for i in range(16):\n",
        "    ax=plt.subplot(4,4,i+1)\n",
        "    plt.imshow(image[i]/255.)\n",
        "    plt.title(CLASS_NAMES[tf.argmax(label[i], axis=0).numpy()])\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "bkQWVMpVX1tA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augment_layers = tf.keras.Sequential([\n",
        "    RandomRotation(factor=(-0.025, 0.025)),\n",
        "    RandomFlip(mode='horizontal'),\n",
        "    RandomContrast(factor=0.1)\n",
        "])\n",
        "\n",
        "def augment_layer(image, label):\n",
        "  return augment_layers(image, training=True), label"
      ],
      "metadata": {
        "id": "u4YQKi0VvJVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def box(lamda):\n",
        "  r_x =  tf.cast(tfp.distributions.Uniform(0,CONFIGURATION[\"IM_SIZE\"]).sample(1)[0], dtype= tf.int32)\n",
        "  r_y = tf.cast(tfp.distributions.Uniform(0,CONFIGURATION[\"IM_SIZE\"]).sample(1)[0], dtype= tf.int32)\n",
        "\n",
        "  r_w = tf.cast(CONFIGURATION[\"IM_SIZE\"]*tf.math.sqrt(1-lamda), dtype=tf.int32)\n",
        "  r_h = tf.cast(CONFIGURATION[\"IM_SIZE\"]*tf.math.sqrt(1-lamda), dtype=tf.int32)\n",
        "\n",
        "  r_x = tf.clip_by_value(r_x - r_w//2, 0, CONFIGURATION[\"IM_SIZE\"])\n",
        "  r_y = tf.clip_by_value(r_y - r_h//2, 0, CONFIGURATION[\"IM_SIZE\"])\n",
        "\n",
        "  x_b_r = tf.clip_by_value(r_x + r_w//2, 0, CONFIGURATION[\"IM_SIZE\"])\n",
        "  y_b_r = tf.clip_by_value(r_y + r_h//2, 0, CONFIGURATION[\"IM_SIZE\"])\n",
        "\n",
        "  r_w = y_b_r - r_y\n",
        "  if (r_w == 0):\n",
        "    r_w = 1\n",
        "  r_h = x_b_r - r_x\n",
        "  if (r_h == 0):\n",
        "    r_h = 1\n",
        "\n",
        "  return r_x, r_y, r_w, r_h"
      ],
      "metadata": {
        "id": "q0uvRtONyQUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cut_mixup(train_dataset_1, train_dataset_2):\n",
        "  (image_1,label_1), (image_2, label_2) = train_dataset_1, train_dataset_2\n",
        "\n",
        "  lamda = tfp.distributions.Beta(0.2, 0.2)\n",
        "  lamda = lamda.sample(1)[0]\n",
        "\n",
        "  r_y, r_x, r_h, r_w = box(lamda)\n",
        "\n",
        "  crop_2= tf.image.crop_to_bounding_box(image_2, r_y, r_x, r_h, r_w)\n",
        "  pad_2 = tf.image.pad_to_bounding_box(crop_2, r_y, r_x, CONFIGURATION[\"IM_SIZE\"],CONFIGURATION[\"IM_SIZE\"])\n",
        "  crop_1 = tf.image.crop_to_bounding_box(image_1, r_y, r_x, r_h, r_w)\n",
        "  pad_1 = tf.image.pad_to_bounding_box(crop_1, r_y, r_x, CONFIGURATION[\"IM_SIZE\"],CONFIGURATION[\"IM_SIZE\"])\n",
        "\n",
        "  image = image_1 - pad_1 + pad_2\n",
        "\n",
        "  lamda = 1 - (r_h*r_w)/(CONFIGURATION[\"IM_SIZE\"]*CONFIGURATION[\"IM_SIZE\"])\n",
        "  label = lamda*tf.cast(label_1, dtype=tf.float64) + (1-lamda)*tf.cast(label_2, dtype=tf.float64)\n",
        "\n",
        "  return image, label"
      ],
      "metadata": {
        "id": "RpM79s2Lzfn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_1  = train_dataset.shuffle(buffer_size=8, reshuffle_each_iteration=True).map(augment_layer)\n",
        "train_dataset_2  = train_dataset.shuffle(buffer_size=8, reshuffle_each_iteration=True).map(augment_layer)\n",
        "\n",
        "mixed_dataset = tf.data.Dataset.zip((train_dataset_1, train_dataset_2))\n"
      ],
      "metadata": {
        "id": "HSs-Fz9JzBDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = (train_dataset.map(augment_layer, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE))\n",
        "# training_dataset = (mixed_dataset.map(cut_mixup, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE))\n",
        "validation_dataset = (val_dataset.prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "# training_dataset = train_dataset.map(augment_layer, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "# validation_dataset = (val_dataset)"
      ],
      "metadata": {
        "id": "5AQ_0fznZnCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,12))\n",
        "\n",
        "for image, label in training_dataset.take(1):\n",
        "  for i in range(16):\n",
        "    ax=plt.subplot(4,4,i+1)\n",
        "    plt.imshow(image[i]/255.)\n",
        "    plt.title(CLASS_NAMES[tf.argmax(label[i], axis=0).numpy()])\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "K3r5QcuIx2Au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = training_dataset.unbatch()\n",
        "validation_dataset = validation_dataset.unbatch()"
      ],
      "metadata": {
        "id": "YMRS4ZIv9iKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_example(image, label):\n",
        "\n",
        "  bytes_feature = Feature(\n",
        "      bytes_list=BytesList(value=[image]))\n",
        "\n",
        "  int_feature = Feature(\n",
        "      int64_list=Int64List(value=[label]))\n",
        "\n",
        "  example = Example(\n",
        "      features=Features(feature={\n",
        "          'images': bytes_feature,\n",
        "          'labels': int_feature,\n",
        "      }))\n",
        "\n",
        "  return example.SerializeToString()"
      ],
      "metadata": {
        "id": "o6C3ArKB-RZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_SHARDS = 10\n",
        "PATH = 'tfrecords/shard_{:02d}.tfrecord'"
      ],
      "metadata": {
        "id": "8iDUL-hK-Rdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def encode_image(image, label):\n",
        "#   image = tf.image.convert_image_dtype(image, dtype=tf.uint8)\n",
        "#   image = tf.io.encode_jpeg(image)\n",
        "#   return image,label\n",
        "\n",
        "def encode_image(image, label):\n",
        "  image = tf.image.convert_image_dtype(image, dtype=tf.uint8)\n",
        "  image = tf.io.encode_jpeg(image)\n",
        "  return image,int(tf.argmax(label))"
      ],
      "metadata": {
        "id": "u7Yq54EmMkee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_dataset = (\n",
        "  training_dataset\n",
        "  .map(encode_image)\n",
        ")"
      ],
      "metadata": {
        "id": "o9mMP2EYMuxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for shard_number in range(NUM_SHARDS):\n",
        "\n",
        "  sharded_dataset = (\n",
        "      encoded_dataset\n",
        "      .shard(NUM_SHARDS, shard_number)\n",
        "      .as_numpy_iterator()\n",
        "  )\n",
        "\n",
        "  with tf.io.TFRecordWriter(PATH.format(shard_number)) as file_writer:\n",
        "    for encoded_image, encoded_label in sharded_dataset:\n",
        "\n",
        "      example = create_example(encoded_image, encoded_label)\n",
        "      file_writer.write(example)"
      ],
      "metadata": {
        "id": "Dnlnp25jKHlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recons_dataset = tf.data.TFRecordDataset(filenames=[PATH.format(p) for p in range(NUM_SHARDS)])"
      ],
      "metadata": {
        "id": "L6IEPXUZjKIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_tfrecords(example):\n",
        "\n",
        "  feature_description = {\n",
        "      'images': tf.io.FixedLenFeature([], tf.string),\n",
        "      'labels': tf.io.FixedLenFeature([], tf.int64)\n",
        "  }\n",
        "\n",
        "  example = tf.io.parse_single_example(example, feature_description)\n",
        "  example['images'] = tf.io.decode_jpeg(example['images'], channels=3)\n",
        "\n",
        "  return example['images'], example['labels']"
      ],
      "metadata": {
        "id": "mEb-ewd6lQH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_dataset = recons_dataset.map(parse_tfrecords).batch(CONFIGURATION[\"BATCH_SIZE\"]).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "j2mPsN4QljoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_dataset.take(1)"
      ],
      "metadata": {
        "id": "FpnJyhd_0mMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resize_rescale_layer = tf.keras.Sequential([\n",
        "    Resizing(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]),\n",
        "    Rescaling(1./255)\n",
        "])"
      ],
      "metadata": {
        "id": "o_f0H2JRaLQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lenet_model = tf.keras.Sequential([\n",
        "    InputLayer(input_shape=(None, None, 3)),\n",
        "    resize_rescale_layer,\n",
        "\n",
        "    Conv2D(filters = CONFIGURATION[\"N_FILTERS\"], kernel_size = CONFIGURATION[\"KERNEL_SIZE\"],\n",
        "           strides = CONFIGURATION[\"N_STRIDES\"], padding = 'valid', activation = 'relu', kernel_regularizer=L2(CONFIGURATION[\"REGULARIZATION_RATE\"])),\n",
        "    BatchNormalization(),\n",
        "    MaxPool2D(pool_size = CONFIGURATION[\"POOL_SIZE\"], strides =  CONFIGURATION[\"N_STRIDES\"] * 2),\n",
        "    Dropout(rate=CONFIGURATION[\"DROPOUT_RATE\"]),\n",
        "\n",
        "    Conv2D(filters = CONFIGURATION[\"N_FILTERS\"]* 2 + 4, kernel_size = CONFIGURATION[\"KERNEL_SIZE\"],\n",
        "           strides = CONFIGURATION[\"N_STRIDES\"], padding = 'valid', activation = 'relu', kernel_regularizer=L2(CONFIGURATION[\"REGULARIZATION_RATE\"])),\n",
        "    BatchNormalization(),\n",
        "    MaxPool2D(pool_size = CONFIGURATION[\"POOL_SIZE\"], strides =  CONFIGURATION[\"N_STRIDES\"] * 2),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(CONFIGURATION[\"N_DENSE_1\"], activation = 'relu', kernel_regularizer=L2(CONFIGURATION[\"REGULARIZATION_RATE\"])),\n",
        "    BatchNormalization(),\n",
        "    Dropout(CONFIGURATION[\"DROPOUT_RATE\"]),\n",
        "    Dense(CONFIGURATION[\"N_DENSE_2\"], activation = 'relu', kernel_regularizer=L2(CONFIGURATION[\"REGULARIZATION_RATE\"])),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Dense(CONFIGURATION[\"NUM_CLASSES\"], activation = 'softmax')\n",
        "])\n",
        "\n",
        "lenet_model.summary()"
      ],
      "metadata": {
        "id": "G6KKmAMBbIeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomConv2D(Layer):\n",
        "  def __init__(self, n_filters, kernel_size, n_strides, padding = 'valid'):\n",
        "    super(CustomConv2D, self).__init__(name = 'custom_conv2d')\n",
        "\n",
        "    self.conv = Conv2D(\n",
        "        filters = n_filters,\n",
        "        kernel_size = kernel_size,\n",
        "        activation = 'relu',\n",
        "        strides = n_strides,\n",
        "        padding = padding)\n",
        "\n",
        "    self.batch_norm = BatchNormalization()\n",
        "\n",
        "  def call(self, x, training = True):\n",
        "\n",
        "    x = self.conv(x)\n",
        "    x = self.batch_norm(x, training)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "Lgs5QBPR8Wdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(Layer):\n",
        "  def __init__(self, n_channels, n_strides = 1):\n",
        "    super(ResidualBlock, self).__init__(name = 'res_block')\n",
        "\n",
        "    self.dotted = (n_strides != 1)\n",
        "\n",
        "    self.custom_conv_1 = CustomConv2D(n_channels, 3, n_strides, padding = \"same\")\n",
        "    self.custom_conv_2 = CustomConv2D(n_channels, 3, 1, padding = \"same\")\n",
        "\n",
        "    self.activation = Activation('relu')\n",
        "\n",
        "    if self.dotted:\n",
        "      self.custom_conv_3 = CustomConv2D(n_channels, 1, n_strides)\n",
        "\n",
        "  def call(self, input, training):\n",
        "\n",
        "    x = self.custom_conv_1(input, training)\n",
        "    x = self.custom_conv_2(x, training)\n",
        "\n",
        "    if self.dotted:\n",
        "      x_add = self.custom_conv_3(input, training)\n",
        "      x_add = Add()([x, x_add])\n",
        "    else:\n",
        "      x_add = Add()([x, input])\n",
        "\n",
        "    return self.activation(x_add)\n"
      ],
      "metadata": {
        "id": "Xz23O9cy37A2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet34(Model):\n",
        "  def __init__(self,):\n",
        "    super(ResNet34, self).__init__(name = 'resnet_34')\n",
        "\n",
        "    self.conv_1 = CustomConv2D(64, 7, 2, padding = 'same')\n",
        "    self.max_pool = MaxPooling2D(3,2)\n",
        "\n",
        "    self.conv_2_1 = ResidualBlock(64)\n",
        "    self.conv_2_2 = ResidualBlock(64)\n",
        "    self.conv_2_3 = ResidualBlock(64)\n",
        "\n",
        "    self.conv_3_1 = ResidualBlock(128, 2)\n",
        "    self.conv_3_2 = ResidualBlock(128)\n",
        "    self.conv_3_3 = ResidualBlock(128)\n",
        "    self.conv_3_4 = ResidualBlock(128)\n",
        "\n",
        "    self.conv_4_1 = ResidualBlock(256, 2)\n",
        "    self.conv_4_2 = ResidualBlock(256)\n",
        "    self.conv_4_3 = ResidualBlock(256)\n",
        "    self.conv_4_4 = ResidualBlock(256)\n",
        "    self.conv_4_5 = ResidualBlock(256)\n",
        "    self.conv_4_6 = ResidualBlock(256)\n",
        "\n",
        "    self.conv_5_1 = ResidualBlock(512, 2)\n",
        "    self.conv_5_2 = ResidualBlock(512)\n",
        "    self.conv_5_3 = ResidualBlock(512)\n",
        "\n",
        "    self.global_pool = GlobalAveragePooling2D()\n",
        "\n",
        "    self.fc_3 = Dense(CONFIGURATION[\"NUM_CLASSES\"], activation = 'softmax')\n",
        "\n",
        "  def call(self, x, training = True):\n",
        "    x = self.conv_1(x)\n",
        "    x = self.max_pool(x)\n",
        "\n",
        "    x = self.conv_2_1(x, training)\n",
        "    x = self.conv_2_2(x, training)\n",
        "    x = self.conv_2_3(x, training)\n",
        "\n",
        "    x = self.conv_3_1(x, training)\n",
        "    x = self.conv_3_2(x, training)\n",
        "    x = self.conv_3_3(x, training)\n",
        "    x = self.conv_3_4(x, training)\n",
        "\n",
        "    x = self.conv_4_1(x, training)\n",
        "    x = self.conv_4_2(x, training)\n",
        "    x = self.conv_4_3(x, training)\n",
        "    x = self.conv_4_4(x, training)\n",
        "    x = self.conv_4_5(x, training)\n",
        "    x = self.conv_4_6(x, training)\n",
        "\n",
        "    x = self.conv_5_1(x, training)\n",
        "    x = self.conv_5_2(x, training)\n",
        "    x = self.conv_5_3(x, training)\n",
        "\n",
        "    x = self.global_pool(x)\n",
        "\n",
        "    return self.fc_3(x)"
      ],
      "metadata": {
        "id": "uiCn-Lh2zew4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_34 = ResNet34()\n",
        "resnet_34(tf.zeros([1, 256, 256, 3]), training = True) # for building resnet model -> For seeing summary\n",
        "resnet_34.summary()"
      ],
      "metadata": {
        "id": "daJbvB4F9vjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "backbone = tf.keras.applications.efficientnet.EfficientNetB4(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"], 3),\n",
        ")"
      ],
      "metadata": {
        "id": "5L47Z6fuHZ2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "backbone.trainable = False\n",
        "# backbone.trainable = True"
      ],
      "metadata": {
        "id": "AlybckTRIkeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    Input(shape=(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"], 3)),\n",
        "    backbone,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(CONFIGURATION[\"N_DENSE_1\"], activation = 'relu'),\n",
        "    BatchNormalization(),\n",
        "    Dense(CONFIGURATION[\"N_DENSE_2\"], activation = 'relu'),\n",
        "    Dense(CONFIGURATION[\"NUM_CLASSES\"], activation = 'softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "YMVEro5RIyOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = Input(shape=(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"], 3))\n",
        "\n",
        "x = backbone(input, training = False)\n",
        "\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(CONFIGURATION[\"N_DENSE_1\"], activation = 'relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(CONFIGURATION[\"N_DENSE_2\"], activation = 'relu')(x)\n",
        "x = Dense(CONFIGURATION[\"NUM_CLASSES\"], activation = 'softmax')(x)\n",
        "\n",
        "model = Model(input, x)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "j673Z9I7NOq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_callback = ModelCheckpoint(\n",
        "    'best_weights',\n",
        "    monitor='val_accuracy',\n",
        "    mode = 'max',\n",
        "    verbose=1,\n",
        "    save_best_only=True,\n",
        "    )"
      ],
      "metadata": {
        "id": "1JzDq-4B-3LC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = CategoricalCrossentropy()\n",
        "metrics = [CategoricalAccuracy(name=\"accuracy\"), TopKCategoricalAccuracy(k=2, name=\"top_k_accuracy\")]"
      ],
      "metadata": {
        "id": "PLmaH5juTyM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_sample_0 = 1525 # angary\n",
        "n_sample_1 = 3019 # happy\n",
        "n_sample_2 = 2255 # sad"
      ],
      "metadata": {
        "id": "UOfOfsvrbEvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6799 -> total inputs\n",
        "class_weights = {0:6799/n_sample_0, 1:6799/n_sample_1, 2:6799/n_sample_2}"
      ],
      "metadata": {
        "id": "7Pb6ojM8aQ3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=Adam(learning_rate=CONFIGURATION[\"LEARNING_RATE\"]/100), #use smaller learning rate during fie tuning of pre train model\n",
        "                    loss=loss_function,\n",
        "                    metrics=metrics)"
      ],
      "metadata": {
        "id": "IngPnHs2P50G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(training_dataset, validation_data=validation_dataset, epochs=CONFIGURATION[\"N_EPOCHS\"], verbose=1, class_weight=class_weights)"
      ],
      "metadata": {
        "id": "S0tRX6J-awMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend([\"train_loss\", \"val_loss\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OVzTr30QbvPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend([\"train_accuracy\", \"val_accuracy\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4dwSHd4ecI_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lenet_model.evaluate(validation_dataset)"
      ],
      "metadata": {
        "id": "vAsDrya9cVjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image = cv2.imread(\"/content/dataset/Emotions Dataset/Emotions Dataset/test/happy/109046.jpg_brightness_1.jpg\")\n",
        "\n",
        "im = tf.constant(test_image, dtype=tf.float32)\n",
        "im = tf.expand_dims(im, axis=0)\n",
        "\n",
        "CLASS_NAMES[(tf.argmax(lenet_model(im), axis=-1)).numpy()[0]]"
      ],
      "metadata": {
        "id": "vCwUxEE5dTm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,12))\n",
        "\n",
        "for image, label in val_dataset.take(1):\n",
        "  for i in range(16):\n",
        "    ax=plt.subplot(4,4,i+1)\n",
        "    plt.imshow(image[i]/255.)\n",
        "    plt.title(\"Predicted Label:\" + CLASS_NAMES[(tf.argmax(lenet_model(im), axis=-1)).numpy()[0]] + \"\\n\" + \"True Label:\" + CLASS_NAMES[tf.argmax(label[i], axis=0).numpy()])\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "P9pZvqaJeLcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = []\n",
        "predicted = []\n",
        "\n",
        "for image, label in val_dataset:\n",
        "  predicted.append(lenet_model(image))\n",
        "  labels.append(label.numpy())"
      ],
      "metadata": {
        "id": "E--n5bbJi73u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.concatenate([np.argmax(labels[:-1], axis=-1).flatten(), np.argmax(labels[-1], axis=-1).flatten()]))\n",
        "print(np.concatenate([np.argmax(predicted[:-1], axis=-1).flatten(), np.argmax(predicted[-1], axis=-1).flatten()]))"
      ],
      "metadata": {
        "id": "BUGEWhssnxrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label = np.concatenate([np.argmax(labels[:-1], axis=-1).flatten(), np.argmax(labels[-1], axis=-1).flatten()])\n",
        "pred  = np.concatenate([np.argmax(predicted[:-1], axis=-1).flatten(), np.argmax(predicted[-1], axis=-1).flatten()])"
      ],
      "metadata": {
        "id": "CnSrwaaOp77R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(label, pred)\n",
        "print(cm)\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "\n",
        "sns.heatmap(cm, annot=True)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")"
      ],
      "metadata": {
        "id": "UnJ-bLzdqHJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_backbone = tf.keras.applications.vgg16.VGG16(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=(CONFIGURATION[\"IM_SIZE\"],CONFIGURATION[\"IM_SIZE\"],3),\n",
        ")"
      ],
      "metadata": {
        "id": "6obIF_f0WH16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_backbone.summary()"
      ],
      "metadata": {
        "id": "EGEIEHKMWe8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_conv(layer_name):\n",
        "  if \"conv\" in layer_name:\n",
        "    return True\n",
        "  return False"
      ],
      "metadata": {
        "id": "eZXg5r-aaKLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_maps = [layer.output for layer in vgg_backbone.layers[1:] if is_conv(layer.name)]\n",
        "feature_map_model = Model(inputs = vgg_backbone.input, outputs=feature_maps)\n",
        "\n",
        "feature_map_model.summary()"
      ],
      "metadata": {
        "id": "b-9Rhv-TWhDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image = cv2.imread(\"/content/dataset/Emotions Dataset/Emotions Dataset/test/happy/100610.jpg_brightness_1.jpg\")\n",
        "test_image = cv2.resize(test_image, (CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]))\n",
        "\n",
        "im = tf.constant(test_image, dtype=tf.float32)\n",
        "im = tf.expand_dims(im, axis=0)\n",
        "\n",
        "f_maps = feature_map_model.predict(im)"
      ],
      "metadata": {
        "id": "ng3f1--rX0xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(f_maps)"
      ],
      "metadata": {
        "id": "wGEsHEy1ZvbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(f_maps)):\n",
        "  print(f_maps[i].shape)"
      ],
      "metadata": {
        "id": "40k9whavZVQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(f_maps)):\n",
        "  plt.figure(figsize=(256,256))\n",
        "  f_size = f_maps[i].shape[1]\n",
        "  n_channels = f_maps[i].shape[3]\n",
        "  joint_maps = np.ones((f_size, f_size*n_channels))\n",
        "\n",
        "  axs = plt.subplot(len(f_maps), 1, i+1)\n",
        "  for j in range(n_channels):\n",
        "    joint_maps[:, f_size*j:f_size*(j+1)] = f_maps[i][...,j]\n",
        "\n",
        "  plt.imshow(joint_maps[:,0:512])\n",
        "  plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "gMa6CDQfZi8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "backbone = tf.keras.applications.efficientnet.EfficientNetB5(\n",
        "    include_top = False,\n",
        "    weights='imagenet',\n",
        "    input_shape=(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"], 3),\n",
        "    )\n",
        "backbone.trainable=False"
      ],
      "metadata": {
        "id": "hzXqXRgUfbOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = backbone.output\n",
        "\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense( CONFIGURATION[\"N_DENSE_1\"], activation = \"relu\")(x)\n",
        "x = Dense( CONFIGURATION[\"N_DENSE_2\"], activation = \"relu\")(x)\n",
        "output = Dense( CONFIGURATION[\"NUM_CLASSES\"], activation = \"softmax\")(x)\n",
        "\n",
        "pretrained_model = Model(backbone.inputs, output)\n",
        "pretrained_model.summary()"
      ],
      "metadata": {
        "id": "EHEilAhDiQj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image = cv2.imread(\"/content/dataset/Emotions Dataset/Emotions Dataset/test/happy/109046.jpg_brightness_1.jpg\")\n",
        "test_image = cv2.resize(test_image, (CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]))\n",
        "\n",
        "im = tf.constant(test_image, dtype=tf.float32)\n",
        "im = tf.expand_dims(im, axis=0)\n"
      ],
      "metadata": {
        "id": "Mq8_SOBQimtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(im)"
      ],
      "metadata": {
        "id": "qXu0ZjGei1vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "last_conv_layer_name = \"top_activation\"\n",
        "last_conv_layer = pretrained_model.get_layer(last_conv_layer_name)\n",
        "last_conv_layer_model = Model(pretrained_model.inputs, last_conv_layer.output)"
      ],
      "metadata": {
        "id": "qRiUIqvijPPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "last_conv_layer_model.summary()"
      ],
      "metadata": {
        "id": "jBlKUxqLmHJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in pretrained_model.layers:\n",
        "  print(layer.name)"
      ],
      "metadata": {
        "id": "6Cau-A3iq5IT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_layer_names = [\n",
        " \"global_average_pooling2d_5\",\n",
        " \"dense_15\",\n",
        " \"dense_16\",\n",
        " \"dense_17\"\n",
        "]"
      ],
      "metadata": {
        "id": "udh83-Rwmlka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_input = Input(shape=(8,8,2048))\n",
        "x = classifier_input\n",
        "for layer_name in classifier_layer_names:\n",
        " x = pretrained_model.get_layer(layer_name)(x)\n",
        "classifier_model = Model(classifier_input, x)"
      ],
      "metadata": {
        "id": "eSSyXry_nMI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.GradientTape() as tape:\n",
        "  last_conv_layer_output = last_conv_layer_model(im)\n",
        "  preds = classifier_model(last_conv_layer_output)\n",
        "  top_pred_index = tf.argmax(preds[0])\n",
        "  print(top_pred_index)\n",
        "  top_class_channel = preds[:, top_pred_index]\n",
        "\n",
        "grads = tape.gradient(top_class_channel, last_conv_layer_output)"
      ],
      "metadata": {
        "id": "TFeuGP-0oemJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grads.shape"
      ],
      "metadata": {
        "id": "WpEricH7q2IJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pooled_grads = tf.reduce_mean(grads, axis=(0,1,2)).numpy()"
      ],
      "metadata": {
        "id": "zcwSrOourkhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pooled_grads.shape"
      ],
      "metadata": {
        "id": "sh5lV4hJryq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "last_conv_layer_output = last_conv_layer_output.numpy()[0]\n",
        "for i in range(2048):\n",
        "  last_conv_layer_output[:, :, i] *= pooled_grads[i]"
      ],
      "metadata": {
        "id": "jveEvZbfr8an"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "last_conv_layer_output.shape"
      ],
      "metadata": {
        "id": "e7SvjcMBse10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "heatmap = np.sum(last_conv_layer_output, axis=-1)"
      ],
      "metadata": {
        "id": "OMc7QdBgsmLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "heatmap=tf.nn.relu(heatmap)\n",
        "plt.matshow(heatmap)"
      ],
      "metadata": {
        "id": "hqTiXkwDstDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resized_heatmap=cv2.resize(np.array(heatmap),(256,256))\n",
        "plt.matshow(resized_heatmap*255+im[0,:,:,0]/255)"
      ],
      "metadata": {
        "id": "xSVw57tJs48-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = Input(shape=(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"], 3))\n",
        "\n",
        "y_1 = model(input)\n",
        "y_2 = pretrained_model(input)\n",
        "\n",
        "output = 0.5*y_1 + 0.5*y_2\n",
        "\n",
        "ensemble_model = Model(inputs=input, outputs=output)"
      ],
      "metadata": {
        "id": "gw8ciWqtWZKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_model.compile(optimizer=Adam(learning_rate=CONFIGURATION[\"LEARNING_RATE\"]),\n",
        "                    loss=loss_function,\n",
        "                    metrics=metrics)"
      ],
      "metadata": {
        "id": "cKuCHytSXYCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image = cv2.imread(\"/content/dataset/Emotions Dataset/Emotions Dataset/test/happy/109046.jpg_brightness_1.jpg\")\n",
        "test_image = cv2.resize(test_image, (CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]))\n",
        "\n",
        "im = tf.expand_dims(test_image, axis=0)\n",
        "\n",
        "patches = tf.image.extract_patches(\n",
        "    images=im,\n",
        "    sizes=[1, 16, 16, 1],\n",
        "    strides=[1, 16, 16, 1],\n",
        "    rates=[1, 1, 1, 1],\n",
        "    padding='VALID',\n",
        ")"
      ],
      "metadata": {
        "id": "2IGRxtI0DNTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(patches.shape)\n",
        "patches = tf.reshape(patches, (patches.shape[0],256, 768))\n",
        "print(patches.shape)"
      ],
      "metadata": {
        "id": "jhMeWTx_D4GP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "\n",
        "for i in range(patches.shape[1]):\n",
        "    ax = plt.subplot(16,16, i+1)\n",
        "    plt.imshow(tf.reshape(patches[0,i,:], (16, 16, 3)))\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "f-Sa5RMSEBAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEncoder(Layer):\n",
        "  def __init__(self, N_PATCHES, HIDDEN_SIZE):\n",
        "    super(PatchEncoder, self).__init__(name = 'patch_encoder')\n",
        "\n",
        "    self.linear_projection = Dense(HIDDEN_SIZE)\n",
        "    self.positional_embedding = Embedding(N_PATCHES, HIDDEN_SIZE)\n",
        "    self.N_PATCHES = N_PATCHES\n",
        "\n",
        "  def call(self, x):\n",
        "    patches = tf.image.extract_patches(\n",
        "      images=x,\n",
        "      sizes=[1, 16, 16, 1],\n",
        "      strides=[1, 16, 16, 1],\n",
        "      rates=[1, 1, 1, 1],\n",
        "      padding='VALID',\n",
        "      )\n",
        "    patches = tf.reshape(patches, (tf.shape(patches)[0] ,256, patches.shape[-1]))\n",
        "\n",
        "    embedding_input = tf.range(start=0, limit=self.N_PATCHES, delta=1)\n",
        "    output =  self.linear_projection(patches) + self.positional_embedding(embedding_input,)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "z0NmxgxXFrPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patch_enc = PatchEncoder(256, 768)\n",
        "patch_enc(tf.zeros([2, 256, 256, 3]))"
      ],
      "metadata": {
        "id": "8L1XLKByPZuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(Layer):\n",
        "  def __init__(self, N_HEADS, HIDDEN_SIZE):\n",
        "    super(TransformerEncoder, self).__init__(name = 'transformer_encoder')\n",
        "\n",
        "    self.layer_norm_1 = LayerNormalization()\n",
        "    self.layer_norm_2 = LayerNormalization()\n",
        "\n",
        "    self.multihead_att = MultiHeadAttention(N_HEADS, HIDDEN_SIZE)\n",
        "\n",
        "    self.dense_1 = Dense(HIDDEN_SIZE, activation = tf.nn.gelu)\n",
        "    self.dense_2 = Dense(HIDDEN_SIZE, activation = tf.nn.gelu)\n",
        "\n",
        "  def call(self, input):\n",
        "    x_1 = self.layer_norm_1(input)\n",
        "    x_1 =  self.multihead_att(x_1, x_1)\n",
        "\n",
        "    x_1 = Add()([x_1, input])\n",
        "\n",
        "    x_2 = self.layer_norm_2(x_1)\n",
        "    x_2 = self.dense_1(x_2)\n",
        "    output = self.dense_2(x_2)\n",
        "\n",
        "    output = Add()([output, x_1])\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "s725-_mZQDQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trans_enc = TransformerEncoder(8, 768)\n",
        "trans_enc(tf.zeros([1, 256, 768]))"
      ],
      "metadata": {
        "id": "JQvq0jyUTvF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(Model):\n",
        "  def __init__(self, N_HEADS, HIDDEN_SIZE, N_PATCHES, N_LAYERS, N_DENSE_UNIT):\n",
        "    super(ViT, self).__init__(name = 'vision_transformer')\n",
        "\n",
        "    self.N_LAYERS = N_LAYERS\n",
        "\n",
        "    self.patch_encoder = PatchEncoder(N_PATCHES, HIDDEN_SIZE)\n",
        "    self.transformer_encoder = [TransformerEncoder(N_HEADS, HIDDEN_SIZE) for _ in range(N_LAYERS)]\n",
        "\n",
        "    self.dense_1 = Dense(N_DENSE_UNIT, tf.nn.gelu)\n",
        "    self.dense_2 = Dense(N_DENSE_UNIT, tf.nn.gelu)\n",
        "    self.dense_3 = Dense(CONFIGURATION[\"NUM_CLASSES\"], activation=\"softmax\")\n",
        "\n",
        "  def call(self, input, training = True):\n",
        "    x = self.patch_encoder(input)\n",
        "\n",
        "    for i in range(self.N_LAYERS):\n",
        "      x = self.transformer_encoder[i](x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = self.dense_1(x)\n",
        "    x = self.dense_2(x)\n",
        "\n",
        "    return self.dense_3(x)\n"
      ],
      "metadata": {
        "id": "YMmWc_CLUOXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit = ViT(8, 768, 256, 4, 1024)\n",
        "vit(tf.zeros([32, 256, 256, 3]))"
      ],
      "metadata": {
        "id": "tkmK6HEuYWaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit.summary()"
      ],
      "metadata": {
        "id": "6atkykpRYtrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit.compile(optimizer=Adam(learning_rate=CONFIGURATION[\"LEARNING_RATE\"]),\n",
        "                    loss=loss_function,\n",
        "                    metrics=metrics)"
      ],
      "metadata": {
        "id": "2fVHobJBYx3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit.fit(training_dataset, validation_data=validation_dataset, epochs=CONFIGURATION[\"N_EPOCHS\"], verbose=1)"
      ],
      "metadata": {
        "id": "DQyI-iUfagH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "ai7dsg_ibNsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTConfig, ViTModel\n",
        "\n",
        "# Initializing a ViT vit-base-patch16-224 style configuration\n",
        "configuration = ViTConfig(hidden_size=144)\n",
        "\n",
        "# Initializing a model (with random weights) from the vit-base-patch16-224 style configuration\n",
        "model = ViTModel(configuration)\n",
        "\n",
        "# Accessing the model configuration\n",
        "configuration = model.config"
      ],
      "metadata": {
        "id": "UcKIL4oMeHGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "configuration"
      ],
      "metadata": {
        "id": "9_JNLxT5eHPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resize_rescale_hf = tf.keras.Sequential([\n",
        "    Resizing(224, 224),\n",
        "    Rescaling(1./255),\n",
        "    Permute((3,1,2))\n",
        "])"
      ],
      "metadata": {
        "id": "nsBcFdl1tWW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoImageProcessor, TFViTModel\n",
        "\n",
        "base_model = TFViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "inputs = Input(shape=(256, 256, 3))\n",
        "\n",
        "x = resize_rescale_hf(inputs)\n",
        "x = base_model.vit(x)[0][:, 0, :] # we are only intrested in class embbeding output\n",
        "\n",
        "output = Dense(CONFIGURATION[\"NUM_CLASSES\"], activation = \"softmax\")(x)\n",
        "\n",
        "hf_model = Model(inputs=inputs, outputs=output)"
      ],
      "metadata": {
        "id": "hNoZEhbJeHVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ArtmbhqJOilD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y7bhMUlbOin4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "URUFh6VTOiq7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}